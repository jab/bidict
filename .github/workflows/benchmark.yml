name: benchmark

"on":
  push:
    branches:
      - main
      - dev
      - deps
  pull_request:
    branches:
      - main
  workflow_dispatch:
    inputs:
      ref:
        description: (optional) ref to benchmark

env:
  FORCE_COLOR: "1"
  PYTHONHASHSEED: "42"
  BASELINE_URL: https://github.com/jab/bidict/releases/download/microbenchmarks/GHA-linux-cachegrind-x86_64-CPython-3.12.2-baseline.json

jobs:
  benchmark:
    runs-on: ubuntu-24.04
    steps:
      - name: install valgrind
        uses: awalsh128/cache-apt-pkgs-action@5902b33ae29014e6ca012c5d8025d4346556bd40
        with:
          packages: valgrind
      - name: check out source
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683
        with:
          fetch-depth: 0
      - name: set up Python
        uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065
        with:
          # Pin to micro-release for better reproducibility.
          # When upgrading to a new Python version, remember to upload new associated baseline
          # benchmark results here: https://github.com/jab/bidict/releases/edit/microbenchmarks
          python-version: '3.12.2'
      - name: set up cached uv
        uses: hynek/setup-cached-uv@757bedc3f972eb7227a1aa657651f15a8527c817
      - name: create and activate virtualenv
        run: |
          uv sync --only-group=test
          source .venv/bin/activate
          echo PATH="$PATH" >> "$GITHUB_ENV"
      - name: install the version of bidict to benchmark
        run: |
          git checkout ${{ github.event.inputs.ref || github.sha }}
          # install a non-editable version of bidict at the revision under test
          uv pip install --no-deps .
          # restore the current revision so we use its version of tests/microbenchmarks.py
          git checkout ${{ github.sha }}
          # move aside the 'bidict' subdirectory to make sure we always import the installed version
          mv -v bidict src
      - name: download baseline benchmark results
        run: |
          curl -L -s -o baseline.json "$BASELINE_URL"
          line1=$(head -n1 baseline.json)
          [ "$line1" = "{" ]
      - name: benchmark and compare to baseline
        run: |
          ./cachegrind.py pytest -n0 \
              --benchmark-autosave \
              --benchmark-columns=min,rounds,iterations \
              --benchmark-disable-gc \
              --benchmark-group-by=name \
              --benchmark-compare=baseline.json \
              tests/microbenchmarks.py
      - name: archive benchmark results
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02
        with:
          name: microbenchmark results
          path: .benchmarks
          if-no-files-found: error

permissions:
  contents: read
